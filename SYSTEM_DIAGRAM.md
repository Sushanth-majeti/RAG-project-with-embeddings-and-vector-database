# RAG Evaluation Project - System Overview

## ğŸ—ï¸ Complete System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         RAG EVALUATION PIPELINE                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STAGE 1: DOCUMENT INGESTION
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DocumentLoader (document_loader.py)                                     â”‚
â”‚ â”œâ”€ PDF extraction (PyPDF2)                                             â”‚
â”‚ â”œâ”€ DOCX parsing (python-docx)                                          â”‚
â”‚ â”œâ”€ Excel reading (openpyxl)                                            â”‚
â”‚ â”œâ”€ PowerPoint extraction (python-pptx)                                 â”‚
â”‚ â””â”€ Markdown reading (raw text)                                         â”‚
â”‚                                                                         â”‚
â”‚ INPUT:  projects/ folder (any of above formats)                        â”‚
â”‚ OUTPUT: List[(source_file, text_content)]                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
STAGE 2: QUERY LOADING
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Load data/queries.json                                                  â”‚
â”‚ 15 evaluation queries with:                                            â”‚
â”‚ â”œâ”€ query text                                                          â”‚
â”‚ â”œâ”€ expected keywords                                                   â”‚
â”‚ â””â”€ expected source patterns                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
STAGE 3: TEXT CHUNKING (PARALLEL)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Execute all 5 strategies independently:                                  â”‚
â”‚                                                                          â”‚
â”‚  Strategy 1       Strategy 2       Strategy 3       Strategy 4           â”‚
â”‚  FixedSize        Recursive        Structure-Aware   Hybrid              â”‚
â”‚  512 tok, 100     Paraâ†’Sentâ†’Fxd   By Headings       Struct+Recursive    â”‚
â”‚  overlap          overlap          Merge small       fallback            â”‚
â”‚  â–¼                â–¼                â–¼                â–¼                    â”‚
â”‚  Chunks[]         Chunks[]         Chunks[]         Chunks[]             â”‚
â”‚                                                                          â”‚
â”‚  Strategy 5                                                              â”‚
â”‚  Table-Aware                                                             â”‚
â”‚  Preserve tables as atomic chunks                                        â”‚
â”‚  â–¼                                                                       â”‚
â”‚  Chunks[]                                                                â”‚
â”‚                                                                          â”‚
â”‚ OUTPUT: Dict[strategy_name] = List[Chunk]                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
STAGE 4: EMBEDDING GENERATION
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ For each chunking strategy:                                              â”‚
â”‚   For each chunk:                                                        â”‚
â”‚     For each embedding model:                                            â”‚
â”‚       Generate embedding (normalized L2 vectors)                         â”‚
â”‚                                                                          â”‚
â”‚  Model 1: all-MiniLM-L6-v2        (384-dim) âš¡âš¡âš¡ fast                 â”‚
â”‚  Model 2: all-mpnet-base-v2       (768-dim) âš¡âš¡ balanced              â”‚
â”‚  Model 3: intfloat/e5-base-v2     (768-dim) âš¡âš¡ strong                â”‚
â”‚  Model 4: BAAI/bge-base-en-v1.5   (768-dim) âš¡âš¡ bilingual            â”‚
â”‚  Model 5: intfloat/e5-large-v2    (1024-dim) âš¡ best quality           â”‚
â”‚                                                                          â”‚
â”‚ Total combinations: 5 strategies Ã— 5 models = 25 embedding configs      â”‚
â”‚                                                                          â”‚
â”‚ OUTPUT: Dict[strategy][model] = np.ndarray (n_chunks, dimension)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
STAGE 5: VECTOR INDEXING (Qdrant)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Create 25 collections (one per strategyÃ—model combination)               â”‚
â”‚                                                                          â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚ â”‚ fixed_size_all-MiniLM-L6-v2 (cosine similarity)               â”‚     â”‚
â”‚ â”‚ â”œâ”€ Point 0: [vector] + {metadata}                            â”‚     â”‚
â”‚ â”‚ â”œâ”€ Point 1: [vector] + {metadata}                            â”‚     â”‚
â”‚ â”‚ â””â”€ Point N: [vector] + {metadata}                            â”‚     â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                                          â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚ â”‚ fixed_size_all-mpnet-base-v2 (cosine similarity)              â”‚     â”‚
â”‚ â”‚ â”œâ”€ Point 0: [vector] + {metadata}                            â”‚     â”‚
â”‚ â”‚ â””â”€ ...                                                         â”‚     â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                                          â”‚
â”‚ ... (23 more collections)                                                â”‚
â”‚                                                                          â”‚
â”‚ Storage: ./qdrant_storage/ (local, in-memory friendly)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
STAGE 6: RETRIEVAL & EVALUATION
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ For each configuration (25 total):                                       â”‚
â”‚   For each query (15 total):                                             â”‚
â”‚     1. Embed query with same model                                       â”‚
â”‚     2. Search Qdrant collection (cosine similarity)                      â”‚
â”‚     3. Retrieve top-10 results                                           â”‚
â”‚     4. Evaluate each result:                                             â”‚
â”‚        - Is source file correct?                                         â”‚
â”‚        - Are keywords present?                                           â”‚
â”‚     5. Calculate metrics:                                                â”‚
â”‚        - Top-1 Accuracy: rank-1 is relevant? (0/1)                      â”‚
â”‚        - Top-3 Accuracy: any of rank 1-3 relevant? (0/1)                â”‚
â”‚        - MRR: 1/rank of first relevant (0-1)                            â”‚
â”‚        - Avg Similarity: mean cosine of relevant (0-1)                  â”‚
â”‚                                                                          â”‚
â”‚ Total queries: 25 configs Ã— 15 queries = 375 retrieval operations       â”‚
â”‚ Total metrics: 25 Ã— 4 = 100 metric values                               â”‚
â”‚                                                                          â”‚
â”‚ OUTPUT: Dict[config] = List[Dict[metric_values]]                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
STAGE 7: AGGREGATION & RANKING
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ For each configuration:                                                  â”‚
â”‚   Average metrics across 15 queries:                                     â”‚
â”‚   - avg_top_1_accuracy                                                   â”‚
â”‚   - avg_top_3_accuracy                                                   â”‚
â”‚   - avg_mrr                                                              â”‚
â”‚   - avg_similarity                                                       â”‚
â”‚   - combined_score = avg_top_3 + avg_mrr                                 â”‚
â”‚                                                                          â”‚
â”‚ Create ranking (sorted by combined_score descending):                    â”‚
â”‚   Rank 1: highest score â† BEST CONFIGURATION                            â”‚
â”‚   Rank 2: second highest                                                 â”‚
â”‚   ...                                                                    â”‚
â”‚   Rank 25: lowest score                                                  â”‚
â”‚                                                                          â”‚
â”‚ OUTPUT: DataFrame[25 rows Ã— 7 cols]                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
STAGE 8: REPORTING
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Generate outputs:                                                        â”‚
â”‚                                                                          â”‚
â”‚ 1. results/results.csv                                                   â”‚
â”‚    â”œâ”€ 25 rows (configurations)                                           â”‚
â”‚    â”œâ”€ 7 columns (metrics)                                                â”‚
â”‚    â””â”€ Sorted by combined_score                                           â”‚
â”‚                                                                          â”‚
â”‚ 2. results/results.json                                                  â”‚
â”‚    â”œâ”€ configurations array                                               â”‚
â”‚    â””â”€ summary_stats object                                               â”‚
â”‚                                                                          â”‚
â”‚ 3. Console output                                                        â”‚
â”‚    â”œâ”€ Results table                                                      â”‚
â”‚    â”œâ”€ Best by each metric                                                â”‚
â”‚    â””â”€ FINAL RECOMMENDATION                                               â”‚
â”‚                                                                          â”‚
â”‚ Example output:                                                          â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚ â”‚ BEST OVERALL COMBINATION                            â”‚                 â”‚
â”‚ â”‚ Chunking: recursive                                 â”‚                 â”‚
â”‚ â”‚ Embedding: e5-large-v2                              â”‚                 â”‚
â”‚ â”‚ Top-3: 0.967, MRR: 0.833, Score: 1.800             â”‚                 â”‚
â”‚ â”‚ Reason: Highest combined score                       â”‚                 â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Data Flow Example

```
INPUT TEXT:
"Our annual revenue was $50M with profits of $10M. 
 This represents 20% growth over last year."

     â”‚
     â–¼ (Chunking Strategy: Recursive)

CHUNKS:
[
  Chunk {
    id: "file_recursive_0",
    content: "Our annual revenue was $50M with profits of $10M.",
    source_file: "annual.pdf",
    metadata: {level: 'sentence', ...}
  },
  Chunk {
    id: "file_recursive_1", 
    content: "This represents 20% growth over last year.",
    source_file: "annual.pdf",
    metadata: {level: 'sentence', ...}
  }
]

     â”‚
     â–¼ (Embedding Model: e5-large-v2)

EMBEDDINGS:
[
  [0.123, -0.456, ..., 0.789],  â† chunk 0 (1024-dim, normalized)
  [0.120, -0.450, ..., 0.785]   â† chunk 1 (1024-dim, normalized)
]

     â”‚
     â–¼ (Indexing to Qdrant)

COLLECTION: recursive_e5-large-v2
[
  Point {
    id: 0,
    vector: [0.123, -0.456, ..., 0.789],
    payload: {chunk_id, content, source_file, ...}
  },
  Point {
    id: 1,
    vector: [0.120, -0.450, ..., 0.785],
    payload: {chunk_id, content, source_file, ...}
  }
]

     â”‚
     â–¼ (Query: "What was the annual revenue?")

QUERY EMBEDDING:
[0.122, -0.455, ..., 0.788]  â† same dimension, same model

     â”‚
     â–¼ (Cosine Similarity Search)

RESULTS:
[
  (chunk_0, score=0.97, metadata),   â† Rank 1 â­
  (chunk_1, score=0.85, metadata),   â† Rank 2
  ...
]

     â”‚
     â–¼ (Evaluation)

METRICS:
- Is rank-1 from annual.pdf? âœ“ YES
- Does rank-1 contain "revenue"? âœ“ YES
- Top-1 Accuracy: 1.0 âœ“
- Top-3 Accuracy: 1.0 âœ“
- MRR: 1/1 = 1.0 âœ“
- Avg Similarity: 0.97 âœ“

     â”‚
     â–¼ (Aggregation across 15 queries)

FINAL METRICS FOR THIS CONFIG:
{
  chunking_strategy: "recursive",
  embedding_model: "e5-large-v2",
  top_1_accuracy: 0.93,
  top_3_accuracy: 0.97,
  mrr: 0.87,
  avg_similarity: 0.71,
  combined_score: 1.84
}
```

---

## ğŸ¯ Configuration Matrix (25 Total)

```
              â”‚   MiniLM   â”‚  MPNet  â”‚  E5-base â”‚  E5-large â”‚   BGE  â”‚
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
Fixed-size    â”‚     1      â”‚    2    â”‚    3     â”‚     4     â”‚   5    â”‚
Recursive     â”‚     6      â”‚    7    â”‚    8     â”‚     9     â”‚   10   â”‚
Structure-Aw  â”‚    11      â”‚   12    â”‚   13     â”‚    14     â”‚   15   â”‚
Hybrid        â”‚    16      â”‚   17    â”‚   18     â”‚    19     â”‚   20   â”‚
Table-Aware   â”‚    21      â”‚   22    â”‚   23     â”‚    24     â”‚   25   â”‚
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Each cell = one Qdrant collection with independent evaluation
```

---

## ğŸ“ˆ Metrics Hierarchy

```
Per-Query Metrics (calculated per query)
â”‚
â”œâ”€ Top-1 Accuracy: Is rank-1 relevant?
â”œâ”€ Top-3 Accuracy: Any of rank 1-3 relevant?
â”œâ”€ MRR: 1 / rank_of_first_relevant
â””â”€ Avg Similarity: Mean cosine of relevant results

     â”‚
     â–¼ (Average across 15 queries)

Per-Configuration Metrics
â”‚
â”œâ”€ avg_top_1_accuracy    (0-1)
â”œâ”€ avg_top_3_accuracy    (0-1)
â”œâ”€ avg_mrr               (0-1)
â”œâ”€ avg_similarity        (0-1)
â””â”€ combined_score = avg_top_3 + avg_mrr  (0-2)

     â”‚
     â–¼ (Rank by combined_score)

FINAL RANKING (25 configurations sorted)
```

---

## ğŸ”„ Execution Timeline

```
Start
  â”‚
  â”œâ”€ [1 sec]   Load documents (from projects/)
  â”œâ”€ [<1 sec]  Load queries (from data/queries.json)
  â”œâ”€ [1-2 sec] Chunking (5 strategies Ã— documents)
  â”œâ”€ [30-60s]  Embedding (5 models Ã— chunks)
  â”‚            â”œâ”€ First 10s: Model download (~2GB) [one-time]
  â”‚            â””â”€ Remaining: Inference
  â”œâ”€ [5-10 sec] Indexing (25 collections to Qdrant)
  â”œâ”€ [10-20 sec] Retrieval & Evaluation (25 Ã— 15 queries)
  â”œâ”€ [5-10 sec] Aggregation & Ranking
  â”œâ”€ [<1 sec]  Report generation
  â”‚
  â””â”€ COMPLETE
  
  Total: ~1-2 minutes (first run ~12 min with model download)
```

---

## ğŸ’¾ Storage & Memory

```
Memory Usage:
- Embeddings: ~10-50 MB per model per configuration
  (1000 chunks Ã— 384-1024 dims Ã— 4 bytes)
- Qdrant: ~100-200 MB total
- Results: < 1 MB

Disk Usage:
- Model files: ~2.5 GB (one-time, cached)
- Qdrant storage: 100-200 MB
- Results: < 1 MB
- Project: ~ 2.5 GB total
```

---

## ğŸ”‘ Key Components

| Component | File | Purpose |
|-----------|------|---------|
| DocumentLoader | document_loader.py | Extract text from files |
| ChunkingStrategies | chunking.py | Split docs 5 different ways |
| EmbeddingPipeline | embeddings.py | Generate vectors for text |
| QdrantManager | vector_db.py | Index & search vectors |
| EvaluationMetrics | evaluation.py | Calculate accuracy metrics |
| Utils | utils.py | Helpers & data classes |
| Orchestrator | main.py | Run full pipeline |

---

## âœ¨ Design Patterns Used

1. **Strategy Pattern**: Different chunking/embedding strategies
2. **Pipeline Pattern**: Stage-by-stage execution
3. **Factory Pattern**: Create objects by name
4. **Observer Pattern**: Logging at each stage
5. **Aggregator Pattern**: Combine metrics from queries

---

**This architecture enables comprehensive comparison of RAG strategies!** ğŸš€
